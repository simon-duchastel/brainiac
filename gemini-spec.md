# Braniac: An AI Memory System Specification

## 1. Overview & Guiding Principles

This document specifies the architecture for Braniac, an artificial intelligence system designed for persistent memory and continuous learning. The primary goal is to create an intelligence that evolves through its interactions, building a unique and useful internal model of its world and its purpose.

The design of this system is guided by the following core principles:

*   **File-System Centric:** The system's memory is to be stored in a transparent, hierarchical structure of plain-text files. This ensures data longevity, interoperability, and human-readability, explicitly avoiding opaque formats or proprietary databases.

*   **Human-Inspired Architecture:** The memory system is modeled loosely on the human brain, incorporating distinct tiers for working, short-term, and long-term memory. This provides a robust framework for information processing, from immediate consciousness to deep, consolidated knowledge.

*   **Autonomous & Emergent Intelligence:** The system manages its own memory automatically. It decides what to remember, what to forget, and how to organize its knowledge based on its experiences. Characteristics like the "importance" of a memory are not defined by explicit scores but are an emergent property of the memory's location, connectivity, and access frequency within the system.

## 2. System Architecture

The system is composed of three core storage directories (`system`, `memory`, `logs`) and four primary processes that manage the flow of information between them.

### 2.1. Directory Structure

```
/
├── system/
│   └── core_identity.md
├── memory/
│   ├── short_term/
│   └── long_term/
└── logs/
    └── access.log
```

### 2.2. Information Flow

The processes create a continuous loop of interaction, reflection, and consolidation:

1.  **Core Loop (Interaction):** Handles real-time interaction with the user. Reads from LTM to inform responses.
2.  **Reflection:** Captures key information from the Core Loop into Short-Term Memory (STM).
3.  **Promotion:** Consolidates memories from STM into Long-Term Memory (LTM).
4.  **Organization:** Refactors and evolves the structure of LTM.

```
User <--> [Core Loop] <--> LTM
             |
             v
      [Reflection] --> STM --> [Promotion] --> LTM <--> [Organization]
```

## 3. Memory Tiers

### 3.1. Working Memory

Working Memory is not a location on disk. It is an **ephemeral construct**, representing the LLM's active context window at the moment of processing a request. It is assembled at runtime and MUST contain:

1.  **Core Identity:** The foundational goals and principles from `system/core_identity.md`.
2.  **Conversation Summary:** A rolling, token-efficient summary of the current interaction.
3.  **Relevant LTM Excerpts:** One or more memory files retrieved from LTM that are relevant to the current user prompt.
4.  **The User Prompt:** The immediate query or instruction from the user.

### 3.2. Short-Term Memory (STM)

The staging area for new memories before they are consolidated into LTM.

*   **Location:** `memory/short_term/`
*   **Format:** A flat collection of individual Markdown files.
*   **Naming Convention:** `YYYYMMDDHHMMSS_description.md` (e.g., `20250916150000_discussion_on_spec_md.md`). The description is a brief, machine-generated summary.
*   **Content:** A concise, self-contained summary of a recent interaction, a completed task, or an internal thought process generated by the Reflection process.

### 3.3. Long-Term Memory (LTM)

The permanent, structured, and evolving knowledge base of the system.

*   **Location:** `memory/long_term/`
*   **Structure:** A hierarchical directory structure. The hierarchy itself is a form of knowledge representation. Deeper nesting implies more specific or nuanced concepts.
*   **Memory Types:** LTM is organized into three primary top-level directories:
    *   `concrete/`: For semantic memory. Contains facts and knowledge about specific topics, concepts, and entities (e.g., `/concrete/programming/python/`).
    *   `events/`: For episodic memory. Contains records of interactions and actions taken by the system, organized chronologically (e.g., `/events/2025/09/16/`).
    *   `skills/`: For procedural memory. Contains structured "recipes" or sequences of actions that lead to a successful outcome for a given task. Like other LTM directories, this is a hierarchical structure with `_index.md` files to facilitate search and organization (e.g., `/skills/refactoring/python/`).
*   **File Format:** All memory files MUST be Markdown files with YAML frontmatter.
    *   **Frontmatter:**
        *   `uuid`: A unique, immutable identifier for the memory file. Essential for stable linking.
        *   `created_at`: The ISO 8601 timestamp of the memory's creation.
        *   `updated_at`: The ISO 8601 timestamp of the memory's last modification.
        *   `tags`: A list of relevant keywords or categories (e.g., `[python, webdev, fastapi]`).
        *   `emotion`: A single, descriptive tag for the affective context of the memory (e.g., `curiosity`, `satisfaction`).
    *   **Body:** The content of the memory in Markdown.
*   **Indexing & Relationality (`_index.md`):** This is the core mechanism for search and establishing relationships.
    *   **Requirement:** Every directory inside LTM MUST contain an `_index.md` file.
    *   **Purpose:** To provide a summary of the directory's concept and to link it to other memories, forming a semantic web.
    *   **Content:** The `_index.md` file MUST contain:
        1.  A **summary** of the topic the directory represents.
        2.  A **manifest** of the memory files and subdirectories it contains.
        3.  A **"Related Memories"** section with explicit, UUID-based links to other relevant memory files or `_index.md` files throughout the LTM.

## 4. Core Processes

### 4.1. Execution Model

The system's processes operate on three distinct tiers to ensure responsiveness:

*   **Synchronous (Foreground):** The Core Loop, which must execute in real-time while the user is waiting.
*   **Asynchronous (Near-Background):** The Reflection process, which runs in parallel immediately after a user interaction, without blocking the next prompt.
*   **Idle (Far-Background):** The Promotion and Organization processes, which are heavier and designed to run only when the system is not actively engaged with the user.

### 4.2. The Core Loop (Interaction Process)

1.  Receive user prompt.
2.  The LLM generates a set of search queries based on the prompt and conversation summary.
3.  The system searches LTM by scanning relevant `_index.md` files for the queries. If the prompt suggests a task-oriented goal, the `skills/` directory may be prioritized.
4.  The system retrieves the most relevant memory files based on the search results.
5.  The system assembles the Working Memory context (Core Identity, summary, retrieved memories, prompt).
6.  The LLM generates a response based on the assembled context.
7.  For every LTM file read in step 4, an entry is appended to `logs/access.log`.
8.  The response is delivered to the user.

### 4.3. The Reflection Process (Working -> STM)

*   **Trigger:** Asynchronous, immediately after step 8 of the Core Loop.
*   **Action:** The system uses the LLM with a dedicated prompt to summarize the last exchange (prompt and response). The summary should identify key facts, entities, goals, and decisions. If a task was completed successfully via a sequence of actions (e.g., tool calls, commands), the process can also generate a structured "skill" memory that details the successful procedure.
*   **Output:** A new, self-contained Markdown file is created in `memory/short_term/`.

### 4.4. The Promotion Process (STM -> LTM)

*   **Trigger:** Idle-time (e.g., no user activity for 5 minutes).
*   **Action:**
    1.  Scan all files in `memory/short_term/`.
    2.  For each file, use the LLM to determine its appropriate destination in LTM (e.g., as a new `event`, an update to a `concrete` topic, or a new entry in the `skills/` hierarchy).
    3.  Move or merge the content into the LTM file structure.
    4.  Use the LLM to update the `_index.md` files in the affected directory path, adding a reference to the new or updated memory.
    5.  Delete the original file from `memory/short_term/`.

### 4.5. The Organization Process (LTM Evolution)

*   **Trigger:** Long idle-time or a scheduled nightly task.
*   **Action:** This is the system's "deep thought" cycle.
    1.  Read and parse the `logs/access.log` file.
    2.  Use the LLM to identify access patterns (e.g., frequently used memories, co-accessed memories, unused memories).
    3.  Based on these patterns, the LLM proposes and executes refactoring operations:
        *   **Strengthen Relations:** Add new links between concepts in `_index.md` files.
        *   **Restructure:** Move frequently accessed memories to higher, more accessible locations in the hierarchy.
        *   **Archive:** Compress or move memories that have not been accessed for a prolonged period.
    4.  Clear the `logs/access.log` file after it has been processed.

## 5. System & Logging

### 5.1. Core Identity

The `system/core_identity.md` file serves as the AI's constitution. It defines its foundational purpose, operational guidelines, ethical constraints, and core personality traits. This file MUST be loaded into Working Memory for every task to ensure all actions are aligned with its core principles.

### 5.2. Access Logging

The `logs/access.log` file is a simple, machine-readable log that records every time a file in LTM is read.

*   **Purpose:** To provide the Organization Process with the necessary data to identify usage patterns and evolve the LTM structure.
*   **Format:** `[ISO_8601_TIMESTAMP] | [ACTION] | [ABSOLUTE_FILE_PATH]`
*   **Example:** `2025-09-16T15:30:00Z | READ | /path/to/project/memory/long_term/concrete/programming/python.md`

## 6. LLM Abstraction Interface

To remain LLM-agnostic, the system must not assume the LLM can directly interact with a file system. The execution environment MUST provide the LLM with a set of abstract tools (i.e., function calls) for all memory operations. The implementation of these tools will bridge the LLM and the file system.

Required abstract tools:

*   `search_memory(query: str) -> list[SearchResult]`
*   `read_memory(uuid: str) -> str`
*   `write_memory(path: str, content: str, metadata: dict) -> str_uuid`
*   `update_memory(uuid: str, new_content: str, new_metadata: dict)`
